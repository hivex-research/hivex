{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "Copyright 2024 The HIVEX Authors. All Rights Reserved.\n",
    "\n",
    "Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "you may not use this file except in compliance with the License.\n",
    "You may obtain a copy of the License at\n",
    "\n",
    "   http://www.apache.org/licenses/LICENSE-2.0\n",
    "\n",
    "Unless required by applicable law or agreed to in writing, software\n",
    "distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or  implied.\n",
    "See the License for the specific language governing permissions and\n",
    "limitations under the License.\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Example Training using Stable Baselines3 and VecEnv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note:\n",
    "1. Install the dependencies as described in the README.md.\n",
    "2. Download or clone the hivex-environments.\n",
    "3. Select the correct kernel for this jupyter notebook at the top right."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\pdsie\\anaconda3\\envs\\hivex-research-rllib\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "2024-08-22 16:30:20,053\tINFO util.py:159 -- Missing packages: ['ipywidgets']. Run `pip install -U ipywidgets`, then restart the notebook server for rich notebook output.\n",
      "2024-08-22 16:30:20,657\tINFO util.py:159 -- Missing packages: ['ipywidgets']. Run `pip install -U ipywidgets`, then restart the notebook server for rich notebook output.\n",
      "2024-08-22 16:30:22,357\tWARNING __init__.py:10 -- PG has/have been moved to `rllib_contrib` and will no longer be maintained by the RLlib team. You can still use it/them normally inside RLlib util Ray 2.8, but from Ray 2.9 on, all `rllib_contrib` algorithms will no longer be part of the core repo, and will therefore have to be installed separately with pinned dependencies for e.g. ray[rllib] and other packages! See https://github.com/ray-project/ray/tree/master/rllib_contrib#rllib-contrib for more information on the RLlib contrib effort.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import yaml\n",
    "from pathlib import Path\n",
    "from typing import Callable, Tuple\n",
    "from gymnasium.spaces import Box, Tuple as TupleSpace, MultiDiscrete\n",
    "import numpy as np\n",
    "\n",
    "# RLlib\n",
    "import ray\n",
    "from ray import tune\n",
    "from ray.rllib.algorithms.ppo import PPOConfig, PPOTorchPolicy\n",
    "from ray.rllib.algorithms.impala import ImpalaConfig, ImpalaTorchPolicy\n",
    "from ray import train\n",
    "from ray.air import CheckpointConfig, RunConfig\n",
    "from ray.rllib.policy.policy import PolicySpec\n",
    "from ray.rllib.utils.typing import PolicyID, AgentID\n",
    "\n",
    "# ML-Agents\n",
    "from mlagents_envs.side_channel.environment_parameters_channel import (\n",
    "    EnvironmentParametersChannel,\n",
    ")\n",
    "from mlagents_envs.side_channel.stats_side_channel import StatsSideChannel\n",
    "\n",
    "# Hivex\n",
    "from hivex.training.framework_wrappers.unity_rllib.environments.hivex_env import (\n",
    "    HivexEnv,\n",
    ")\n",
    "from hivex.training.framework_wrappers.unity_rllib.sidechannels.metrics_sidechannel import (\n",
    "    CustomMetricsCallback,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define global variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The max. number of `step()`s for any episode (per agent) before it'll be reset again automatically.\n",
    "EPISODE_HORIZON = 3000\n",
    "FRAMEWORK = \"torch\"\n",
    "NUM_ROLLOUT_WORKERS = 1\n",
    "# Number of iterations to train.\n",
    "STOP_ITERS = 9999\n",
    "# Number of timesteps to train.\n",
    "STOP_TIMESTEPS = 1800000\n",
    "# Reward at which we stop training.\n",
    "STOP_REWARD = 9999.0\n",
    "DATA_PATH = Path(\"src/data\")\n",
    "LOCAL_MODE = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialize Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_policy_configs_for_game(\n",
    "    name: str, policy: PolicySpec\n",
    ") -> Tuple[dict, Callable[[AgentID], PolicyID]]:\n",
    "    # The RLlib server must know about the Spaces that the Client will be\n",
    "    # using inside Unity3D, up-front.\n",
    "    obs_spaces = {\n",
    "        \"WindFarmControl\": Box(float(\"-inf\"), float(\"inf\"), (6,), dtype=np.float32),\n",
    "        \"WildfireResourceManagement\": Box(\n",
    "            float(\"-inf\"), float(\"inf\"), (7,), dtype=np.float32\n",
    "        ),\n",
    "        \"DroneBasedReforestation\": TupleSpace(\n",
    "            [\n",
    "                Box(float(0), float(1), (16, 16, 1)),\n",
    "                Box(float(\"-inf\"), float(\"inf\"), (20,), dtype=np.float32),\n",
    "            ]\n",
    "        ),\n",
    "        \"OceanPlasticCollection\": TupleSpace(\n",
    "            [\n",
    "                Box(float(0), float(1), (25, 25, 2)),\n",
    "                Box(float(\"-inf\"), float(\"inf\"), (8,), dtype=np.float32),\n",
    "            ]\n",
    "        ),\n",
    "        \"AerialWildfireSuppression\": TupleSpace(\n",
    "            [\n",
    "                Box(float(0), float(1), (42, 42, 3)),\n",
    "                Box(float(\"-inf\"), float(\"inf\"), (8,), dtype=np.float32),\n",
    "            ]\n",
    "        ),\n",
    "    }\n",
    "    action_spaces = {\n",
    "        \"WindFarmControl\": MultiDiscrete([3]),\n",
    "        \"WildfireResourceManagement\": MultiDiscrete([3, 3, 3, 3]),\n",
    "        \"DroneBasedReforestation\": TupleSpace(\n",
    "            [\n",
    "                Box(float(-1), float(1), (3,), dtype=np.float32),\n",
    "                MultiDiscrete([2]),\n",
    "            ]\n",
    "        ),\n",
    "        \"OceanPlasticCollection\": MultiDiscrete([2, 3]),\n",
    "        \"AerialWildfireSuppression\": TupleSpace(\n",
    "            [\n",
    "                Box(float(-1), float(1), (1,), dtype=np.float32),\n",
    "                MultiDiscrete([2]),\n",
    "            ]\n",
    "        ),\n",
    "    }\n",
    "\n",
    "    policies = {\n",
    "        name: PolicySpec(\n",
    "            policy_class=policy,\n",
    "            observation_space=obs_spaces[name],\n",
    "            action_space=action_spaces[name],\n",
    "        ),\n",
    "    }\n",
    "\n",
    "    def policy_mapping_fn(agent_id, episode, worker, **kwargs):\n",
    "        return name\n",
    "\n",
    "    return policies, policy_mapping_fn\n",
    "\n",
    "\n",
    "def train(experiment_config: dict):\n",
    "    channel = EnvironmentParametersChannel()\n",
    "    for key, env_parameter in experiment_config[\"env_parameters\"].items():\n",
    "        channel.set_float_parameter(key, env_parameter)\n",
    "\n",
    "    stats_channel = StatsSideChannel()\n",
    "    if experiment_config[\"policy\"] == \"PPO\":\n",
    "        policy = PPOTorchPolicy\n",
    "        policy_config = PPOConfig\n",
    "    elif experiment_config[\"policy\"] == \"IMPALA\":\n",
    "        policy = ImpalaTorchPolicy\n",
    "        policy_config = ImpalaConfig\n",
    "    else:\n",
    "        raise ValueError(f\"Policy {experiment_config['policy']} not supported.\")\n",
    "    policies, policy_mapping_fn = get_policy_configs_for_game(\n",
    "        experiment_config[\"tag\"], policy\n",
    "    )\n",
    "\n",
    "    param_space = policy_config()\n",
    "\n",
    "    param_space.environment(\n",
    "        \"unity3d\",\n",
    "        env_config={\n",
    "            \"file_name\": experiment_config[\"file_name\"],\n",
    "            \"episode_horizon\": EPISODE_HORIZON,\n",
    "        },\n",
    "    )\n",
    "    param_space.framework(FRAMEWORK)\n",
    "    param_space.rollouts(\n",
    "        num_rollout_workers=NUM_ROLLOUT_WORKERS,\n",
    "        rollout_fragment_length=9000,  # 128,  # in ml agents: time_horizon\n",
    "    )\n",
    "    if experiment_config[\"policy\"] == \"PPO\":\n",
    "        param_space.training(\n",
    "            lr=0.0003,\n",
    "            lambda_=0.95,\n",
    "            gamma=0.995,\n",
    "            sgd_minibatch_size=256,  # in ml agents: batch_size\n",
    "            train_batch_size=9000,  # 2048,  # 4096  # in ml agents: buffer_size\n",
    "            num_sgd_iter=3,\n",
    "            clip_param=0.2,\n",
    "            model={\n",
    "                \"fcnet_hiddens\": [256, 256],\n",
    "            },\n",
    "        )\n",
    "    elif experiment_config[\"policy\"] == \"IMPALA\":\n",
    "        param_space.training(\n",
    "            lr=0.0003,\n",
    "            gamma=0.995,\n",
    "            train_batch_size=9000,  # 2048,  # 4096  # in ml agents: buffer_size\n",
    "            model={\n",
    "                \"fcnet_hiddens\": [256, 256],\n",
    "            },\n",
    "        )\n",
    "    else:\n",
    "        raise ValueError(f\"Policy {experiment_config['policy']} not supported.\")\n",
    "    param_space.multi_agent(policies=policies, policy_mapping_fn=policy_mapping_fn)\n",
    "    # Use GPUs iff `RLLIB_NUM_GPUS` env var set to > 0.\n",
    "    param_space.resources(\n",
    "        num_gpus=int(os.environ.get(\"RLLIB_NUM_GPUS\", \"0\" if LOCAL_MODE else \"1\")),\n",
    "    )\n",
    "    param_space.callbacks(CustomMetricsCallback)\n",
    "\n",
    "    param_space = param_space.to_dict()\n",
    "\n",
    "    stop = {\n",
    "        \"training_iteration\": STOP_ITERS,\n",
    "        \"timesteps_total\": STOP_TIMESTEPS,\n",
    "    }\n",
    "\n",
    "    run_config_dict = {\n",
    "        \"name\": experiment_config[\"name\"],\n",
    "        \"checkpoint_config\": CheckpointConfig(\n",
    "            checkpoint_frequency=50,\n",
    "            checkpoint_at_end=True,\n",
    "        ),\n",
    "        \"verbose\": 2,\n",
    "        \"stop\": stop,\n",
    "    }\n",
    "\n",
    "    run_config_dict.update(experiment_config)\n",
    "\n",
    "    run_config = RunConfig(\n",
    "        name=run_config_dict[\"name\"],\n",
    "        checkpoint_config=run_config_dict[\"checkpoint_config\"],\n",
    "        verbose=run_config_dict[\"verbose\"],\n",
    "        stop=run_config_dict[\"stop\"],\n",
    "        local_dir=\"C:/ray\",\n",
    "    )\n",
    "\n",
    "    tune.register_env(\n",
    "        \"unity3d\",\n",
    "        lambda c: HivexEnv(\n",
    "            run_config=experiment_config,\n",
    "            file_name=experiment_config[\"file_name\"],\n",
    "            episode_horizon=EPISODE_HORIZON,\n",
    "            stop_time_steps=STOP_TIMESTEPS,\n",
    "            side_channel=[channel, stats_channel],\n",
    "        ),\n",
    "    )\n",
    "\n",
    "    # Run the experiment\n",
    "    tuner = tune.Tuner(\n",
    "        experiment_config[\"policy\"],\n",
    "        param_space=param_space,\n",
    "        tune_config=tune.TuneConfig(num_samples=1 if LOCAL_MODE else 10),\n",
    "        run_config=run_config,\n",
    "    )\n",
    "    results = tuner.fit()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-08-22 16:33:26,465\tINFO worker.py:1673 -- Started a local Ray instance.\n",
      "2024-08-22 16:33:30,398\tINFO tune.py:586 -- [output] This uses the legacy output and progress reporter, as Jupyter notebooks are not supported by the new engine, yet. For more information, please see https://github.com/ray-project/ray/issues/36949\n",
      ":task_name:bundle_reservation_check_func\n",
      ":actor_name:PPO\n",
      ":actor_name:RolloutWorker\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<IPython.core.display.HTML object>\n",
      ":task_name:bundle_reservation_check_func\n",
      ":actor_name:PPO\n",
      ":actor_name:RolloutWorker\n",
      "Created UnityEnvironment for port 5006\n",
      "AGENT COUNT: 8\n",
      "###################################################################\n",
      "####################### ENVIRONMENT CREATED #######################\n",
      "###################################################################\n",
      ":actor_name:PPO\n",
      "-------------------------------------------------------------------\n",
      "####################### RESET #####################################\n",
      "-------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      ":actor_name:PPO\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total steps taken: 4999 - last episode total steps taken: 4999 - 8/8 agents done\n",
      "{'WindFarmControl/Individual Performance': 276.775625705719}\n",
      "-------------------------------------------------------------------\n",
      "####################### RESET #####################################\n",
      "-------------------------------------------------------------------\n",
      "total steps taken: 5000 - last episode total steps taken: 0 - 8/8 agents done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-08-22 16:33:52,783\tWARNING tune.py:186 -- Stop signal received (e.g. via SIGINT/Ctrl+C), ending Ray Tune run. This will try to checkpoint the experiment state one last time. Press CTRL+C (or send SIGINT/SIGKILL/SIGTERM) to skip. \n"
     ]
    }
   ],
   "source": [
    "# Wind Farm Control\n",
    "wind_farm_control_config_file = \"../src/hivex/training/examples/rllib_train/configs/wind_farm_control.yml\"\n",
    "# Wildfire Resource Management\n",
    "wildfire_resource_management_config_file = \"../src/hivex/training/examples/rllib_train/configs/wildfire_resource_management.yml\"\n",
    "# Drone Based Reforestation\n",
    "drone_based_reforestation_config_file = \"../src/hivex/training/examples/rllib_train/configs/drone_based_reforestation.yml\"\n",
    "# Ocean Plastic Collection\n",
    "ocean_plastic_collection_config_file = \"../src/hivex/training/examples/rllib_train/configs/ocean_plastic_collection.yml\"\n",
    "# Aerial Wildfire Suppression\n",
    "aerial_wildfire_suppression_config_file = \"../src/hivex/training/examples/rllib_train/configs/aerial_wildfire_suppression.yml\"\n",
    "\n",
    "ray.init(local_mode=LOCAL_MODE)\n",
    "with open(wind_farm_control_config_file, \"r\") as file:\n",
    "    experiment_config = yaml.safe_load(file)\n",
    "\n",
    "experiment_config[\"file_name\"] = f\"../{experiment_config['file_name']}\"\n",
    "\n",
    "train(experiment_config)\n",
    "\n",
    "ray.shutdown()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "hivex-research-stable",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
